{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kochmar mentions several steps required in a typical NLP pipeline, one of them being *Split into words*. Why is this step necessary? Why can we not just feed the text as it is into a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 steps include\n",
    "1. Define classes\n",
    "2. Split into words\n",
    "3. Extract features\n",
    "4. Train classifier\n",
    "5. Test & evaluate\n",
    "\n",
    "Step 2, Split into words, is necessary to identify words in the text. Processing the entire text as one unit is often less useful because it overlooks the specific meanings and functions of each word. Therefore, the text is usually split into words to facilitate the feature-extaction process in Step 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simply splitting on \"words\" (i.e. whitespace) is rarely enough. Consider the sentence below (\"That U.S.A. poster-print costs $12.40...\") and name some problems that arise from splitting on whitespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the text into words based on whitespace ['That', 'U.S.A', 'poster-print', 'costs', '$12.40...'], issues arise since the punctuation marks are not seperated from the words. This will become problematic when extracting the words from the text accurately. The is especially evident with '$12.40...', where the currency symbol '$' is a significant piece of information and should be identified as one token associated with the number, which should also be idenitfied as a single token. In addition, the three dots should ideally be a separate token, as it is a punctuation mark, not part of the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish, experiment with implementing different rules for tokenization. You will see that the \"ruleset\" quickly grows if you want to account for all types of edge cases...\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "def your_rulebased_tokenizer(sentence):\n",
    "    tokens = []\n",
    "    return tokens\n",
    "\n",
    "your_rulebased_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several tokenizers implemented, such as a specific one for Twitter data. Below, indicated by the `TODO`-tag, you should find and import various tokenizers and add them to the list of tokenizers:\n",
    "\n",
    "`tokenizers = [tokenizer1, tokenizer2, ..., tokenizerN]`\n",
    "\n",
    "Tokenize the sentence with at least three different tokenizers supplied by NLTK and comment on your findings. You will find the documentation for NLTK's tokenizers [here](https://www.nltk.org/_modules/nltk/tokenize.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyWhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "WordTokenizer (7 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "WordPunctTokenizer (16 tokens)\n",
      "['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '...']\n",
      "\n",
      "RegExpTokenizer (9 tokens)\n",
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, regexp_tokenize\n",
    "\n",
    "# this is the base class of tokenizers in nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "\n",
    "\n",
    "# this is just a simple example of how a tokenizer can be implemented\n",
    "class MyWhitespaceTokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "# ************************************************************\n",
    "\n",
    "# tokenizer which splits text into words based on punctuation\n",
    "class WordTokenizer(TokenizerI):\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return word_tokenize(text)\n",
    "\n",
    "# tokenizer which splits words from punctuation    \n",
    "class WordPunctTokenizer(TokenizerI):\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return wordpunct_tokenize(text)\n",
    "\n",
    "# tokenizer which splits each sentence    \n",
    "class RegExpTokenizer(TokenizerI):\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return regexp_tokenize(text, pattern=r'\\w+')\n",
    "    \n",
    "# ************************************************************\n",
    "tokenizers = [\n",
    "    MyWhitespaceTokenizer(),\n",
    "    WordTokenizer(),\n",
    "    WordPunctTokenizer(),\n",
    "    RegExpTokenizer()\n",
    "]\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "\n",
    "tokenize(tokenizers, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The findings show that each tokenizer handles the sentence quite differently. The MyWhitespaceTokenizer maintains tokens such as \"U.S.A.\" and \"poster-print\", but tokenizes \"$12.40...\" as a single token. The WordTokenizer and the WordPunctTokenizer are, in contrast to MyWhitespaceTokenizer, able to identify the number and the dots as seperate tokens, in addition to seperating the currency symbol from the number. However, the WordTokenizer identifies the number \"12.40\" as a single token, while the WordPunctTokenizer identifies the first and the second part of the cost as single tokens, \"12\",\".\" and \"40\". In addition, the word \"poster-print\" is a single entity in the WordTokenizer, whereas three tokens in the WordPunctTokenizer. Similarly, \"U.S.A.\" is tokenized as a single entity in the WordTokenizer, compared to as 6 tokens, \"U\", \".\", \"S\", \".\", \"A\", \".\",  in the WordPunctTokenizer. The latter causes the tokens to lose its meaning. The RegExpTokenizer removes all punctuation, and instead only tokenizes the words. \n",
    "\n",
    "In this example, the WordTokenizer seems to be the most appropriate tokenizer as it identifies amounts and compound terms. However, in other applications, the WordPunctTokenizer might be more useful in cases where an analysis of the individidual characters is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language modeling\n",
    "We have now studied the bigger models like BERT and GPT-based language models. A simpler language model, however, can implemented using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is an n-gram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are used in NLP processing to predict text based on some previous context. The \"n\" in n-grams represents the number of characters or words considered as context. A unigram considers a single character or word, a bigram considers two, a trigram three, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use NLTK to print out bigrams and trigrams for the given sentence below. Your function should support any number of N.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2-grams ---\n",
      "('That', 'U.S.A.')\n",
      "('U.S.A.', 'poster-print')\n",
      "('poster-print', 'costs')\n",
      "('costs', '$')\n",
      "('$', '12.40')\n",
      "('12.40', '...')\n",
      "('...', 'I')\n",
      "('I', \"'d\")\n",
      "(\"'d\", 'pay')\n",
      "('pay', '$')\n",
      "('$', '5.00')\n",
      "('5.00', 'for')\n",
      "('for', 'it')\n",
      "('it', '.')\n",
      "--- 3-grams ---\n",
      "('That', 'U.S.A.', 'poster-print')\n",
      "('U.S.A.', 'poster-print', 'costs')\n",
      "('poster-print', 'costs', '$')\n",
      "('costs', '$', '12.40')\n",
      "('$', '12.40', '...')\n",
      "('12.40', '...', 'I')\n",
      "('...', 'I', \"'d\")\n",
      "('I', \"'d\", 'pay')\n",
      "(\"'d\", 'pay', '$')\n",
      "('pay', '$', '5.00')\n",
      "('$', '5.00', 'for')\n",
      "('5.00', 'for', 'it')\n",
      "('for', 'it', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, word_tokenize\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "# tokenize text\n",
    "def tokenize(text: str) -> List[str]:\n",
    "        return word_tokenize(text)\n",
    "\n",
    "# generate and print each n-gram\n",
    "for n in [2, 3]:\n",
    "    print(f\"--- {n}-grams ---\")\n",
    "    for ngram in ngrams(tokenize(sentence), n):\n",
    "        print(ngram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Based on your intuition for language modeling, how can n-grams be used for word predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my intuition, n-grams can be used in character and word predictions by assessing which characters or words often occur together in a similar context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK includes the `FreqDist` class, which produces the frequency distribution of words in a sentence. Use it to print out the two most common words in the text below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  is that \n",
      "   6    5 \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "text = \"That that is is that that is not. Is that it? It is. You sure? Surely it is!\"\n",
    "\n",
    "# count the frequency of each word \n",
    "fdist = FreqDist()\n",
    "\n",
    " # convert each word to lowercase and increment its count in the frequency distribution\n",
    "for word in word_tokenize(text):\n",
    "    fdist[word.lower()] += 1\n",
    "\n",
    "# print the two most frequent words\n",
    "fdist.tabulate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use your n-gram function from question 2.2 to print out the most common trigram of the text in question 2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('That', 'U.S.A.', 'poster-print') \n",
      "                                 1 \n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, word_tokenize\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "# tokenize text\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# create trigrams from the tokenized text\n",
    "trigrams = ngrams(tokenize(sentence), 3)\n",
    "\n",
    "# count the frequency of each trigram\n",
    "fdist = FreqDist(trigrams)\n",
    "\n",
    "# print the most common trigram\n",
    "fdist.tabulate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You may have discovered that you would need to implement some form of preprocessing to get the correct answer to the previous tasks. Preprocessing/cleaning/normalization is often necessary for the desired results. If you were to process the text of a news site or blog post, can you think of some preprocessing steps that would be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to make the text lower-case so all words are treated equal regardless of their case. In addition, punctuation should be removed so words such as \"U.S.A.\" and \"USA\" are treated equally. Spell-checking might also be feasible to perform, to ensure that words with mispellings are not ignored. Words that are contracted, e.g. \"I'd\", should be expanded, such as to \"I would\". Performing stemming and lemmatization would also be useful to identify similar words, e.g. \"costs\" and \"cost\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Representations\n",
    "For more information on word representations, consult the lab description file and course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe the main differences between bag-of-words and one-hot encoding through examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bag-of-words and one-hot encoding are techniques used in text preprocessing. \n",
    "\n",
    "One-hot encoding represents a document as a numeric vector. The vector consists of ones, the \"hot\" values, for every word present in the document, and zeros for any words not present in the document, based on a set of unique words in a corpus. This way, one-hot encoding tracks the presence or absence of words.\n",
    "\n",
    "Bag-of-words, on the other hand, represents a document as a \"bag\" of the unqiue words in the document, where the frequency of the appearance of each word is counted. This technique provides insight into the importance of each word.\n",
    "\n",
    "The example below demonstrates the differences between the two techniques. The one-hot encoding only consists of zeros and ones, whereas the bag-of-words consists of the frequency of a particular word in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: {'is', 'fox', 'often', 'jumps', 'not', 'the', 'in', 'quick', 'rather', 'but', 'are', 'over', 'seen', 'lazy', 'dogs', 'park', 'and', 'cats', 'brown', 'dog', 'energetic'}\n",
      "\n",
      "One-hot encoded vectors:\n",
      "Sentence 1: [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "Sentence 2: [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Sentence 3: [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
      "\n",
      "Bag-of-words vectors:\n",
      "Sentence 1: [0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "Sentence 2: [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Sentence 3: [2, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# code source: https://www.geeksforgeeks.org/one-hot-encoding-in-nlp/\n",
    "\n",
    "# document collection\n",
    "corpus = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "          \"Lazy cats and quick dogs are often seen in the park.\", \n",
    "          \"The dog is not lazy but is rather quick and energetic.\"]\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return regexp_tokenize(text, pattern=r'\\w+')\n",
    "\n",
    "# unique set of words from the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "    for word in tokenize(sentence):\n",
    "        unique_words.add(word.lower())\n",
    "\n",
    "# map each word to an index in a dictionary \n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_to_index[word] = i\n",
    "\n",
    "# initialize lists for one-hot vectors and bag-of-words vectors\n",
    "one_hot = []\n",
    "bag_of_words = []\n",
    "\n",
    "# add each sentence as a vector to the lists\n",
    "for sentence in corpus:\n",
    "    one_hot_vector = [0] * len(unique_words)\n",
    "    bag_of_words_vector = [0] * len(unique_words)\n",
    "\n",
    "    for word in tokenize(sentence):\n",
    "        index = word_to_index[word.lower()]\n",
    "        one_hot_vector[index] = 1\n",
    "        bag_of_words_vector[index] += 1\n",
    "\n",
    "    one_hot.append(one_hot_vector)\n",
    "    bag_of_words.append(bag_of_words_vector)\n",
    "\n",
    "# print results\n",
    "print(\"Unique words:\", unique_words)\n",
    "print(\"\\nOne-hot encoded vectors:\")\n",
    "for i, vector in enumerate(one_hot):\n",
    "    print(f\"Sentence {i+1}: {vector}\")\n",
    "\n",
    "print(\"\\nBag-of-words vectors:\")\n",
    "for i, vector in enumerate(bag_of_words):\n",
    "    print(f\"Sentence {i+1}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the limitations of the above representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both representations suffer from sparsity issues in large datasets due to the potential high volume of zero-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Example of word embedding techniques, such as Word2Vec and GloVe are considered *dense* representations. How do dense word embeddings relate to the *distributional hypothesis*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributional hypothesis states that words that occur in similar contexts usually have similar meanings. This hypothesis is related to dense word embeddings since such techniques generate word embeddings based on the context of a given word and is presented in a vector space where semantically similar words are located close to each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
