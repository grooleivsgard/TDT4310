{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Given the list of pluralized words below, define your own simple word stemmer function or class,  limited to only simple rules and regex. No libraries! It should strip basic endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'deny', 'itemize', 'sensation', 'refer', 'colonize']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "plurals = [\n",
    "    \"flies\",\n",
    "    \"denied\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "]\n",
    "\n",
    "# define rules\n",
    "rules = [\n",
    "            (r'ies$', 'y'),  # flies -> fly\n",
    "            (r'ied$', 'y'),    # denied -> deny\n",
    "            (r'ation$', 'e'), # itemization -> itemize\n",
    "            (r'al$', ''),    # sensational -> sensation\n",
    "            (r'ence$', ''),  # reference -> refer\n",
    "            (r'izer$', 'ize')     # colonizer -> colonize\n",
    "        ]\n",
    "\n",
    "# stemmer implementation\n",
    "def stem(text):\n",
    "    stemmed_words = []\n",
    "    for word in text:\n",
    "        for suffix, replacement in rules:\n",
    "            if re.search(suffix, word):\n",
    "                stemmed_words.append(re.sub(suffix, replacement, word))\n",
    "    return stemmed_words\n",
    "\n",
    "print(stem(plurals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After your initial implementation, run it on the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "new_words = [\n",
    "    \"friendly\",\n",
    "    \"puzzling\",\n",
    "    \"helpful\",\n",
    "]\n",
    "\n",
    "print(stem(new_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizing that fixing future words manually can be problematic, use a desired NLTK stemmer and run it on all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fli', 'deny', 'item', 'sens', 'ref', 'colon', 'friend', 'puzzl', 'help']\n"
     ]
    }
   ],
   "source": [
    "# source: Getting Started with Natural Language Processing, ch. 3\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "all_words = plurals + new_words\n",
    "\n",
    "def stemmer(word_list):\n",
    "\n",
    "    # retrieve stopwords\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "    # use Lancaster Stemmer\n",
    "    st = LancasterStemmer()\n",
    "\n",
    "    # stem words\n",
    "    stemmed_words = [st.stem(word) for word in word_list\n",
    "                    if word.lower() not in stopwords_list and word not in string.punctuation]\n",
    "    return stemmed_words\n",
    "\n",
    "stemmed_word_list = stemmer(all_words)\n",
    "print(stemmed_word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. There are likely a few words in the outputs above that would cause issues in real-world applications. Pick some examples, and show how they are solved with a lemmatizer. Use either spaCy or nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemmer above incorrectly stems multiple words, including \"flies\" to \"fli\" instead of \"fly\", which can result in errors like over-stemming or under-stemming, affecting the accuracy of word analysis. Lemmatization, on the other hand, reduces the word to its root through a linguistic analysis of a word. For example, lemmatization would correctly reduce \"flies\" to \"fly\" by recognizing it as the plural form of \"fly\" as either the noun (the fly) or the verb (to fly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'denied', 'itemization', 'sensational', 'reference', 'colonizer', 'friendly', 'puzzling', 'helpful']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download necessary NLTK data\n",
    "def download_nltk_data(package):\n",
    "    try:\n",
    "        nltk.data.find(package)\n",
    "    except LookupError:\n",
    "        nltk.download(package.split('/')[-1])\n",
    "\n",
    "# initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize words\n",
    "lemmatized_word_list= [lemmatizer.lemmatize(word.lower()) for word in all_words]\n",
    "print(lemmatized_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lemmatization - Practical Example\n",
    "Using the news corpus (subset/category of the Brown corpus), perform common text normalization techniques such as stopword filtering and stemming/lemmatization. Compare the top 10 most common **words** before and after these normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5580), (',', 5188), ('.', 4030), ('of', 2849), ('and', 2146), ('to', 2116), ('a', 1993), ('in', 1893), ('for', 943), ('The', 806)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = brown.words(categories='news')\n",
    "\n",
    "# calculate frequency distribution\n",
    "fdist_initial = FreqDist(words)\n",
    "\n",
    "# print the top 10 most common words\n",
    "print(fdist_initial.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 406), ('would', 246), ('year', 244), ('new', 241), ('one', 221), ('state', 213), ('last', 177), ('two', 174), ('first', 158), ('president', 143)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "words = brown.words(categories='news')\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# filter out stopwords and lemmatize remaining words\n",
    "normalized_words = [lemmatizer.lemmatize(word.lower()) for word in words\n",
    "                    if word.lower() not in stopwords_list and word.isalpha()]\n",
    "\n",
    "# calculate frequency distribution\n",
    "fdist_initial = FreqDist(normalized_words)\n",
    "\n",
    "# print the top 10 most common words\n",
    "print(fdist_initial.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a way to measure the importance of a word in a document.\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the term (word)\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Implement TF-IDF using NLTKs FreqDist (no use of e.g. scikit-learn and other high-level libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def tf(document: List[str], term: str) -> float:\n",
    "    \n",
    "    fdist = FreqDist(document)\n",
    "    term_freq = fdist[term.lower()]/len(document)\n",
    "    \n",
    "    return term_freq\n",
    "\n",
    "\n",
    "def idf(documents: List[List[str]], term: str) -> float:\n",
    "    \n",
    "    term = term.lower()\n",
    "    total_docs = len(documents)\n",
    "    term_docs = sum(1 for doc in documents if term in set(word.lower() for word in doc))\n",
    "\n",
    "    # idf with smoothing to account for cases when the term is not present in any of the docs or in the case of zero documents\n",
    "    inverse_doc_freq = math.log((1 + total_docs) / (1 + term_docs))\n",
    "\n",
    "    return inverse_doc_freq\n",
    "\n",
    "\n",
    "def tf_idf(\n",
    "    all_documents: List[List[str]],\n",
    "    document: List[str],\n",
    "    term: str,\n",
    ") -> float:\n",
    "    \n",
    "    tf_idf = tf(document, term) * idf(all_documents, term)\n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With your TF-IDF function in place, calculate the TF-IDF for the following words in the first document of the news articles found in the Brown corpus: \n",
    "\n",
    "- *the*\n",
    "- *nevertheless*\n",
    "- *highway*\n",
    "- *election*\n",
    "\n",
    "Perform any preprocessing steps you deem necessary. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 0.0\n",
      "nevertheless: 0.0\n",
      "highway: 0.0029400864103517653\n",
      "election: 0.008253604709969881\n"
     ]
    }
   ],
   "source": [
    "fileids = brown.fileids(categories='news')\n",
    "first_doc = list(brown.words(fileids[0]))\n",
    "all_docs = [list(brown.words(fileid)) for fileid in fileids]\n",
    "\n",
    "term_list = ['the', 'nevertheless', 'highway', 'election']\n",
    "\n",
    "for term in term_list:\n",
    "    score = tf_idf(all_docs, first_doc, term)\n",
    "    print(f'{term}: {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to not remove stopwords, such as \"the\", in order to compare the tf-idf scores. The findings show that the tf-idf score for a usually frequent word (\"the\") is much lower than a more unique word. This is because the tf-idf values unique words (eg. \"election\") higher than frequent words (eg. \"nevertheless\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. While TF-IDF is primarily used for information retrieval and text mining, reflect on how TF-IDF could be used in a language modeling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used in a language modeling context such as sentiment analysis by placing more emphasis on less common, but significant, words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You were previously introduced to word representations. TF-IDF can be considered one. What are some differences between the TF-IDF output and one that is computed once from a vocabulary (e.g. one-hot encoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF calculates numerical weights of a word based on its occurrence in a document, relative to its frequency in all documents. This way, more unique words are highlighted. In contrast, one-hot encoding uses binary vectors to represent words which only indicates the presence or absence of a word without any context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Practical Example\n",
    "You will again be looking at specific words for a document, but this time weighted by their TF-IDF scores. Ideally, the scoring should be able to retrieve representative words for this document in context of its document collection or category.\n",
    "\n",
    "You will do the following:\n",
    "- Select a category from the Reuters (news) corpus\n",
    "- Perform preprocessing\n",
    "- Calculate TF-IDF scores\n",
    "- Find the top 5 words for *each document* in a subset of documents in your collection (e.g. 5, 10, ... documents total)\n",
    "- Inspect whether these words make sense for a given document, and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: price: 0.0533, approval: 0.0533, approved: 0.0533, sri: 0.0533, continental: 0.0533\n",
      "Document 2: mln: 0.0800, sown: 0.0509, pct: 0.0423, last: 0.0349, crop: 0.0349\n",
      "Document 3: agent: 0.0960, honduras: 0.0720, pct: 0.0570, better: 0.0480, laydays: 0.0480\n",
      "Document 4: tunisia: 0.1263, french: 0.0947, tender: 0.0722, credits: 0.0631, coface: 0.0631\n",
      "Document 5: flour: 0.1218, iraq: 0.0913, ccc: 0.0609, bonus: 0.0609, bid: 0.0609\n",
      "Document 6: buy: 0.1218, egypt: 0.1218, authorized: 0.1218, ship: 0.0609, existing: 0.0609\n",
      "Document 7: agreement: 0.1019, january: 0.0764, pl: 0.0669, signed: 0.0669, discussing: 0.0669\n",
      "Document 8: hectares: 0.1299, mln: 0.0920, china: 0.0650, henan: 0.0568, pests: 0.0568\n",
      "Document 9: fao: 0.0781, world: 0.0521, mln: 0.0482, output: 0.0390, record: 0.0298\n",
      "Document 10: oil: 0.1065, prices: 0.0838, export: 0.0663, adjusted: 0.0639, follows: 0.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/groelisabeth/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/groelisabeth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk; \n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "category = 'wheat'\n",
    "documents = reuters.fileids(category)\n",
    "\n",
    "# preprocess documents\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc_id in documents[:10]:  # subset of 10 documents\n",
    "    words = [word.lower() for word in word_tokenize(reuters.raw(doc_id)) if word.isalpha()]\n",
    "    preprocessed_docs.append([word for word in words if word not in stopword_list])\n",
    "\n",
    "top_words_per_doc = []\n",
    "\n",
    "for doc in preprocessed_docs:\n",
    "    scores = {}\n",
    "    for word in set(doc):\n",
    "        tf_idf_score = tf_idf(preprocessed_docs, doc, word)\n",
    "        # store the score in the dictionary\n",
    "        scores[word] = tf_idf_score\n",
    "\n",
    "    top_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_words_per_doc.append(top_words)\n",
    "\n",
    "# display top 5 words for each document\n",
    "for i, top_words in enumerate(top_words_per_doc, start=1):\n",
    "    formatted_top_words = ', '.join([f\"{word}: {score:.4f}\" for word, score in top_words])\n",
    "    print(f\"Document {i}: {formatted_top_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe your understanding of POS tagging and its possible use-cases in context of text generation applications/language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging analyzes the syntax of text by assigning each word to a part of speech, such as a noun, verb or adjective. This process aids in the understanding of textual content. It can be applied in text generation to ensure grammatically correct sentences, as well as in language modeling as it aids in distinguishing words that can e.g. be both a noun and a verb (e.g. \"fly\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a UnigramTagger (NLTK) using the Brown corpus. \n",
    "Hint: the taggers in nltk require a list of sentences containing tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/groelisabeth/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/groelisabeth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "trained_tagger = None\n",
    "\n",
    "def tag_sentence(sentence):\n",
    "    global trained_tagger\n",
    "\n",
    "    # train the tagger if it hasn't been trained yet\n",
    "    if trained_tagger is None:\n",
    "        brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "        trained_tagger = UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "    # tokenize if necessary\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # tag the sentence using the trained UnigramTagger\n",
    "    tagged_sentence = trained_tagger.tag(sentence)\n",
    "\n",
    "    return tagged_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use this tagger to tag the text given below. Print out the POS tags for all variants of \"justify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine a situation where you have to explain why you did something – that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\n",
      "\n",
      "[('Imagine', 'VERB'), ('a', 'DET'), ('situation', 'NOUN'), ('where', 'ADV'), ('you', 'PRON'), ('have', 'VERB'), ('to', 'PRT'), ('explain', 'VERB'), ('why', 'ADV'), ('you', 'PRON'), ('did', 'VERB'), ('something', 'NOUN'), ('–', None), ('that', 'ADP'), (\"'s\", None), ('when', 'ADV'), ('you', 'PRON'), ('justify', 'VERB'), ('your', 'DET'), ('actions', 'NOUN'), ('.', '.'), ('So', 'ADV'), (',', '.'), ('let', 'VERB'), (\"'s\", None), ('say', 'VERB'), ('you', 'PRON'), ('made', 'VERB'), ('a', 'DET'), ('decision', 'NOUN'), (';', '.'), ('you', 'PRON'), (',', '.'), ('as', 'ADP'), ('the', 'DET'), ('justifier', None), (',', '.'), ('need', 'VERB'), ('to', 'PRT'), ('give', 'VERB'), ('good', 'ADJ'), ('reasons', 'NOUN'), ('(', '.'), ('justifications', 'NOUN'), (')', '.'), ('for', 'ADP'), ('your', 'DET'), ('choice', 'NOUN'), ('.', '.'), ('You', 'PRON'), ('might', 'VERB'), ('use', 'NOUN'), ('justifying', 'VERB'), ('words', 'NOUN'), ('to', 'PRT'), ('make', 'VERB'), ('your', 'DET'), ('point', 'NOUN'), ('clear', 'ADJ'), ('and', 'CONJ'), ('reasonable', 'ADJ'), ('.', '.'), ('Justifying', None), ('can', 'VERB'), ('be', 'VERB'), ('a', 'DET'), ('bit', 'NOUN'), ('like', 'ADP'), ('saying', 'VERB'), (',', '.'), ('``', '.'), ('Here', 'ADV'), (\"'s\", None), ('why', 'ADV'), ('I', 'PRON'), ('did', 'VERB'), ('what', 'DET'), ('I', 'PRON'), ('did', 'VERB'), ('.', '.'), (\"''\", '.'), ('When', 'ADV'), ('you', 'PRON'), ('justify', 'VERB'), ('things', 'NOUN'), (',', '.'), ('you', 'PRON'), (\"'re\", None), ('basically', 'ADV'), ('providing', 'VERB'), ('the', 'DET'), ('why', 'ADV'), ('behind', 'ADP'), ('your', 'DET'), ('actions', 'NOUN'), ('.', '.'), ('So', 'ADV'), (',', '.'), ('being', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('justifier', None), ('involves', 'VERB'), ('carefully', 'ADV'), ('explaining', 'VERB'), (',', '.'), ('giving', 'VERB'), ('reasons', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('making', 'VERB'), ('sure', 'ADJ'), ('others', 'NOUN'), ('understand', 'VERB'), ('your', 'DET'), ('choices', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Imagine a situation where you have to explain why you did something – that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\n",
    "\"\"\"\n",
    "\n",
    "tagged_sentence = tag_sentence(text)\n",
    "print(text)\n",
    "print(tagged_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Your results may be disappointing. Repeat the same task as above using both the default NLTK pos-tagger and with spaCy. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/groelisabeth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# tokenize the sentence\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# NLTK's default POS tagger\n",
    "nltk_tagged = nltk.pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy in your environment if you haven't already:\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# load spaCys\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process sentence\n",
    "doc = nlp(text)\n",
    "\n",
    "# extract tokens and POS tags\n",
    "spacy_tagged = [(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Imagine', 'VERB'), ('a', 'DET'), ('situation', 'NOUN'), ('where', 'ADV'), ('you', 'PRON'), ('have', 'VERB'), ('to', 'PRT'), ('explain', 'VERB'), ('why', 'ADV'), ('you', 'PRON'), ('did', 'VERB'), ('something', 'NOUN'), ('–', None), ('that', 'ADP'), (\"'s\", None), ('when', 'ADV'), ('you', 'PRON'), ('justify', 'VERB'), ('your', 'DET'), ('actions', 'NOUN'), ('.', '.'), ('So', 'ADV'), (',', '.'), ('let', 'VERB'), (\"'s\", None), ('say', 'VERB'), ('you', 'PRON'), ('made', 'VERB'), ('a', 'DET'), ('decision', 'NOUN'), (';', '.'), ('you', 'PRON'), (',', '.'), ('as', 'ADP'), ('the', 'DET'), ('justifier', None), (',', '.'), ('need', 'VERB'), ('to', 'PRT'), ('give', 'VERB'), ('good', 'ADJ'), ('reasons', 'NOUN'), ('(', '.'), ('justifications', 'NOUN'), (')', '.'), ('for', 'ADP'), ('your', 'DET'), ('choice', 'NOUN'), ('.', '.'), ('You', 'PRON'), ('might', 'VERB'), ('use', 'NOUN'), ('justifying', 'VERB'), ('words', 'NOUN'), ('to', 'PRT'), ('make', 'VERB'), ('your', 'DET'), ('point', 'NOUN'), ('clear', 'ADJ'), ('and', 'CONJ'), ('reasonable', 'ADJ'), ('.', '.'), ('Justifying', None), ('can', 'VERB'), ('be', 'VERB'), ('a', 'DET'), ('bit', 'NOUN'), ('like', 'ADP'), ('saying', 'VERB'), (',', '.'), ('``', '.'), ('Here', 'ADV'), (\"'s\", None), ('why', 'ADV'), ('I', 'PRON'), ('did', 'VERB'), ('what', 'DET'), ('I', 'PRON'), ('did', 'VERB'), ('.', '.'), (\"''\", '.'), ('When', 'ADV'), ('you', 'PRON'), ('justify', 'VERB'), ('things', 'NOUN'), (',', '.'), ('you', 'PRON'), (\"'re\", None), ('basically', 'ADV'), ('providing', 'VERB'), ('the', 'DET'), ('why', 'ADV'), ('behind', 'ADP'), ('your', 'DET'), ('actions', 'NOUN'), ('.', '.'), ('So', 'ADV'), (',', '.'), ('being', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('justifier', None), ('involves', 'VERB'), ('carefully', 'ADV'), ('explaining', 'VERB'), (',', '.'), ('giving', 'VERB'), ('reasons', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('making', 'VERB'), ('sure', 'ADJ'), ('others', 'NOUN'), ('understand', 'VERB'), ('your', 'DET'), ('choices', 'NOUN')]\n",
      "[('Imagine', 'VB'), ('a', 'DT'), ('situation', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('explain', 'VB'), ('why', 'WRB'), ('you', 'PRP'), ('did', 'VBD'), ('something', 'NN'), ('–', 'NN'), ('that', 'WDT'), (\"'s\", 'VBZ'), ('when', 'WRB'), ('you', 'PRP'), ('justify', 'VBP'), ('your', 'PRP$'), ('actions', 'NNS'), ('.', '.'), ('So', 'RB'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('say', 'VB'), ('you', 'PRP'), ('made', 'VBD'), ('a', 'DT'), ('decision', 'NN'), (';', ':'), ('you', 'PRP'), (',', ','), ('as', 'IN'), ('the', 'DT'), ('justifier', 'NN'), (',', ','), ('need', 'VBP'), ('to', 'TO'), ('give', 'VB'), ('good', 'JJ'), ('reasons', 'NNS'), ('(', '('), ('justifications', 'NNS'), (')', ')'), ('for', 'IN'), ('your', 'PRP$'), ('choice', 'NN'), ('.', '.'), ('You', 'PRP'), ('might', 'MD'), ('use', 'VB'), ('justifying', 'VBG'), ('words', 'NNS'), ('to', 'TO'), ('make', 'VB'), ('your', 'PRP$'), ('point', 'NN'), ('clear', 'JJ'), ('and', 'CC'), ('reasonable', 'JJ'), ('.', '.'), ('Justifying', 'VBG'), ('can', 'MD'), ('be', 'VB'), ('a', 'DT'), ('bit', 'NN'), ('like', 'IN'), ('saying', 'VBG'), (',', ','), ('``', '``'), ('Here', 'RB'), (\"'s\", 'VBZ'), ('why', 'WRB'), ('I', 'PRP'), ('did', 'VBD'), ('what', 'WP'), ('I', 'PRP'), ('did', 'VBD'), ('.', '.'), (\"''\", \"''\"), ('When', 'WRB'), ('you', 'PRP'), ('justify', 'VBP'), ('things', 'NNS'), (',', ','), ('you', 'PRP'), (\"'re\", 'VBP'), ('basically', 'RB'), ('providing', 'VBG'), ('the', 'DT'), ('why', 'WRB'), ('behind', 'IN'), ('your', 'PRP$'), ('actions', 'NNS'), ('.', '.'), ('So', 'RB'), (',', ','), ('being', 'VBG'), ('a', 'DT'), ('good', 'JJ'), ('justifier', 'NN'), ('involves', 'VBZ'), ('carefully', 'RB'), ('explaining', 'VBG'), (',', ','), ('giving', 'VBG'), ('reasons', 'NNS'), (',', ','), ('and', 'CC'), ('making', 'VBG'), ('sure', 'JJ'), ('others', 'NNS'), ('understand', 'VBP'), ('your', 'PRP$'), ('choices', 'NNS')]\n",
      "[('\\n', 'SPACE'), ('Imagine', 'VERB'), ('a', 'DET'), ('situation', 'NOUN'), ('where', 'SCONJ'), ('you', 'PRON'), ('have', 'VERB'), ('to', 'PART'), ('explain', 'VERB'), ('why', 'SCONJ'), ('you', 'PRON'), ('did', 'VERB'), ('something', 'PRON'), ('–', 'PUNCT'), ('that', 'PRON'), (\"'s\", 'AUX'), ('when', 'SCONJ'), ('you', 'PRON'), ('justify', 'VERB'), ('your', 'PRON'), ('actions', 'NOUN'), ('.', 'PUNCT'), ('So', 'ADV'), (',', 'PUNCT'), ('let', 'VERB'), (\"'s\", 'PRON'), ('say', 'VERB'), ('you', 'PRON'), ('made', 'VERB'), ('a', 'DET'), ('decision', 'NOUN'), (';', 'PUNCT'), ('you', 'PRON'), (',', 'PUNCT'), ('as', 'ADP'), ('the', 'DET'), ('justifier', 'NOUN'), (',', 'PUNCT'), ('need', 'VERB'), ('to', 'PART'), ('give', 'VERB'), ('good', 'ADJ'), ('reasons', 'NOUN'), ('(', 'PUNCT'), ('justifications', 'NOUN'), (')', 'PUNCT'), ('for', 'ADP'), ('your', 'PRON'), ('choice', 'NOUN'), ('.', 'PUNCT'), ('You', 'PRON'), ('might', 'AUX'), ('use', 'VERB'), ('justifying', 'VERB'), ('words', 'NOUN'), ('to', 'PART'), ('make', 'VERB'), ('your', 'PRON'), ('point', 'NOUN'), ('clear', 'ADJ'), ('and', 'CCONJ'), ('reasonable', 'ADJ'), ('.', 'PUNCT'), ('Justifying', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('a', 'DET'), ('bit', 'NOUN'), ('like', 'ADP'), ('saying', 'VERB'), (',', 'PUNCT'), ('\"', 'PUNCT'), ('Here', 'ADV'), (\"'s\", 'AUX'), ('why', 'SCONJ'), ('I', 'PRON'), ('did', 'VERB'), ('what', 'PRON'), ('I', 'PRON'), ('did', 'VERB'), ('.', 'PUNCT'), ('\"', 'PUNCT'), ('When', 'SCONJ'), ('you', 'PRON'), ('justify', 'VERB'), ('things', 'NOUN'), (',', 'PUNCT'), ('you', 'PRON'), (\"'re\", 'AUX'), ('basically', 'ADV'), ('providing', 'VERB'), ('the', 'PRON'), ('why', 'SCONJ'), ('behind', 'ADP'), ('your', 'PRON'), ('actions', 'NOUN'), ('.', 'PUNCT'), ('So', 'ADV'), (',', 'PUNCT'), ('being', 'AUX'), ('a', 'DET'), ('good', 'ADJ'), ('justifier', 'NOUN'), ('involves', 'VERB'), ('carefully', 'ADV'), ('explaining', 'VERB'), (',', 'PUNCT'), ('giving', 'VERB'), ('reasons', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('making', 'VERB'), ('sure', 'ADJ'), ('others', 'NOUN'), ('understand', 'VERB'), ('your', 'PRON'), ('choices', 'NOUN'), ('\\n', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentence)\n",
    "print(nltk_tagged)\n",
    "print(spacy_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finally, explore more features of the what the spaCy *document* includes related to topics covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition:\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .:\n",
      " - The Fulton County Grand Jury (ORG)\n",
      " - Friday (DATE)\n",
      " - Atlanta (GPE)\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .:\n",
      " - the City Executive Committee (ORG)\n",
      " - the City of Atlanta (GPE)\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .:\n",
      " - September-October (DATE)\n",
      " - Fulton Superior Court (ORG)\n",
      " - Durwood Pye (PERSON)\n",
      " - Ivan Allen Jr. (PERSON)\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .:\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .:\n",
      " - Georgia (GPE)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# use the first 5 sentences from the Brown corpora\n",
    "sentences = brown.sents(categories='news')[:5]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Named Entity Recognition:\")\n",
    "\n",
    "# Named Entity Recognition\n",
    "for sentence in sentences:\n",
    "    sentence_text = \" \".join(sentence)\n",
    "    doc = nlp(sentence_text)\n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    print(f\"{doc}:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - {ent.text} ({ent.label_})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
